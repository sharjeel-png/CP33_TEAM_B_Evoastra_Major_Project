{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1a1cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell A (top) ===\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "# folder for checkpoints\n",
    "os.makedirs('saved_models_tf', exist_ok=True)\n",
    "\n",
    "# Callback: save a .keras single-file each epoch (Keras-3 friendly).\n",
    "# Optionally also try model.export(...) if available (for TF-serving), but .keras is primary.\n",
    "class KerasEpochSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, folder='saved_models_tf', keep_last=3):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        os.makedirs(self.folder, exist_ok=True)\n",
    "        self.keep_last = keep_last\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        base = os.path.join(self.folder, f'epoch_{epoch+1}')\n",
    "        # remove previous directory for this epoch (optional)\n",
    "        if os.path.exists(base):\n",
    "            shutil.rmtree(base)\n",
    "        # try to export TF-style directory (optional)\n",
    "        try:\n",
    "            # available in some Keras 3 builds\n",
    "            self.model.export(base)\n",
    "            print(f\"Exported TF artifact: {base}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        # ALWAYS save a single-file .keras (Keras-3 native)\n",
    "        keras_path = base + '.keras'\n",
    "        try:\n",
    "            self.model.save(keras_path)   # writes single .keras file\n",
    "            print(f\"Saved .keras checkpoint: {keras_path}\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to save .keras checkpoint:\", e)\n",
    "\n",
    "        # optional: keep only last N .keras files to limit disk\n",
    "        files = sorted([p for p in os.listdir(self.folder) if p.endswith('.keras')],\n",
    "                       key=lambda n: os.path.getmtime(os.path.join(self.folder, n)))\n",
    "        if len(files) > self.keep_last:\n",
    "            to_rm = files[:len(files) - self.keep_last]\n",
    "            for rm in to_rm:\n",
    "                try:\n",
    "                    os.remove(os.path.join(self.folder, rm))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "# instantiate and export the callback (use this in model.fit)\n",
    "save_callback = KerasEpochSaver(folder='saved_models_tf', keep_last=3)\n",
    "\n",
    "# CONTROL SWITCH\n",
    "# True  = force fresh start\n",
    "# False = try to auto-resume from latest .keras (preferred)\n",
    "START_FRESH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0493387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Run in terminal or in a notebook cell with a leading !\n",
    "!pip install -q tensorflow pillow tqdm nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a8e288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done. TF version: 2.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yazda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Explanation + imports\n",
    "# Simple explanation: we'll use TensorFlow for the model, PIL for images, nltk for simple text tools.\n",
    "import os, json, random, pickle\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "print(\"All imports done. TF version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce84ebb",
   "metadata": {},
   "source": [
    "Cell 2 — Paths and basic settings\n",
    "\n",
    "What this does: tells the notebook where your images and captions are, and sets a few training parameters.\n",
    "Edit paths here if your folders are in different places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "543f5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Paths & basic settings (edit if your folders are elsewhere)\n",
    "IMAGES_DIR = r'E:\\EVOASTRA INTERNSHIP\\Major_Project [ Caption Generation ]_16_11_2025\\Major_Project_Caption_Generation_118k\\train2017\\train2017'                     # folder with the COCO images\n",
    "ANNOTATIONS_FILE = r'E:\\EVOASTRA INTERNSHIP\\Major_Project [ Caption Generation ]_16_11_2025\\Major_Project_Caption_Generation_118k\\annotations_trainval2017\\annotations\\captions_train2017.json'  # COCO captions json\n",
    "FEATURES_DIR = './features'                           # where we will save image features\n",
    "TOKENIZER_PATH = './tokenizer.pickle'\n",
    "# MODEL_PATH = './caption_model.h5'\n",
    "MODEL_PATH = './caption_model.keras'\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# Simple hyperparameters (beginners can try these)\n",
    "IMG_SIZE = (299, 299)   # input size for InceptionV3\n",
    "MAX_VOCAB = 5000        # max number of words (keeps memory low)\n",
    "MAX_LENGTH = 30         # max caption length (words)\n",
    "EMBED_DIM = 256\n",
    "UNITS = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20              # low number to test quickly; increase later\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Subset option (set True to test quickly)\n",
    "USE_SUBSET = False\n",
    "SUBSET_SIZE = 500       # number of images to use if USE_SUBSET=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134bc68",
   "metadata": {},
   "source": [
    "Cell 3 — Load captions and create a mapping image -> captions\n",
    "\n",
    "What this does: reads the JSON caption file and creates a dictionary where each image filename maps to a list of captions (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ca833dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images available: 118287\n",
      "Example image: 000000203564.jpg\n",
      "First 2 captions: ['a bicycle replica with a clock as the front wheel.', 'the bike has a clock as a tire.']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - load captions and map image file -> list of captions\n",
    "with open(ANNOTATIONS_FILE, 'r') as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "# Map COCO image id -> filename\n",
    "imageid2file = {im['id']: im['file_name'] for im in ann['images']}\n",
    "\n",
    "# Build filename -> captions\n",
    "img2caps = {}\n",
    "for a in ann['annotations']:\n",
    "    fname = imageid2file[a['image_id']]\n",
    "    cap = a['caption'].strip().lower()\n",
    "    img2caps.setdefault(fname, []).append(cap)\n",
    "\n",
    "# Optionally take a small subset for quick testing\n",
    "all_fnames = list(img2caps.keys())\n",
    "if USE_SUBSET:\n",
    "    chosen = set(random.sample(all_fnames, min(SUBSET_SIZE, len(all_fnames))))\n",
    "    img2caps = {k: v for k, v in img2caps.items() if k in chosen}\n",
    "\n",
    "print(\"Number of images available:\", len(img2caps))\n",
    "# Show an example\n",
    "sample = next(iter(img2caps.items()))\n",
    "print(\"Example image:\", sample[0])\n",
    "print(\"First 2 captions:\", sample[1][:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f91a67",
   "metadata": {},
   "source": [
    "Cell 4 — Helper: load image & preprocess for InceptionV3\n",
    "\n",
    "What this does: defines a small function that loads an image, resizes it, and applies InceptionV3 preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "303fffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - helper to load and preprocess a single image\n",
    "def load_and_preprocess_image(path):\n",
    "    img = Image.open(path).convert('RGB').resize(IMG_SIZE)\n",
    "    arr = np.array(img).astype('float32')\n",
    "    arr = preprocess_input(arr)  # scale, normalize like InceptionV3 expects\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7c840",
   "metadata": {},
   "source": [
    "Cell 5 — Extract and save image features (run once)\n",
    "\n",
    "What this does: uses pre-trained InceptionV3 (without the final classification head) to convert each image into a fixed-length feature vector and saves it to disk.\n",
    "Run this cell once — it may take time depending on number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c139f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 118287/118287 [00:06<00:00, 17266.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction done. Saved in ./features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " # Cell 5 - extract pooled features and save as .npy files\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg')  # pooling='avg' -> one vector per image\n",
    "base_model.trainable = False\n",
    "print(\"Feature dimension:\", base_model.output_shape[1])\n",
    "\n",
    "for fname in tqdm(img2caps.keys(), desc=\"Extracting features\"):\n",
    "    image_id = os.path.splitext(fname)[0]\n",
    "    out_path = os.path.join(FEATURES_DIR, image_id + '.npy')\n",
    "    if os.path.exists(out_path):\n",
    "        continue\n",
    "    img_path = os.path.join(IMAGES_DIR, fname)\n",
    "    if not os.path.exists(img_path):\n",
    "        # skip if image file missing\n",
    "        continue\n",
    "    arr = load_and_preprocess_image(img_path)\n",
    "    arr = np.expand_dims(arr, 0)\n",
    "    feat = base_model.predict(arr, verbose=0)[0]   # shape (feature_dim,)\n",
    "    np.save(out_path, feat)\n",
    "print(\"Feature extraction done. Saved in\", FEATURES_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149ff3",
   "metadata": {},
   "source": [
    "Cell 5.1 extracted feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image ID: 000000315805\n",
      "Feature vector shape: (2048,)\n",
      "First 10 feature values: [0.1403391  0.58434105 0.26905352 0.3974986  0.625443   0.21929707\n",
      " 1.0755721  0.31673926 0.01581189 0.31125978]\n",
      "\n",
      "Image ID: 000000162358\n",
      "Feature vector shape: (2048,)\n",
      "First 10 feature values: [0.31865853 0.16279033 0.92625237 0.390826   0.48552263 0.2658648\n",
      " 0.19599834 0.16347247 0.02084418 0.39156142]\n",
      "\n",
      "Image ID: 000000304684\n",
      "Feature vector shape: (2048,)\n",
      "First 10 feature values: [0.20918494 0.15040675 0.2227888  0.02145237 0.0744825  0.64717346\n",
      " 0.6128953  0.17526665 0.4107612  0.4435931 ]\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.1 extracted feature vectors ===\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Pick some random image IDs from train_ids\n",
    "sample_ids = random.sample(train_ids, 3)\n",
    "\n",
    "for img_id in sample_ids:\n",
    "    feat_path = os.path.join(FEATURES_DIR, img_id + '.npy')\n",
    "    if os.path.exists(feat_path):\n",
    "        feat = np.load(feat_path)\n",
    "        print(f\"\\nImage ID: {img_id}\")\n",
    "        print(\"Feature vector shape:\", feat.shape)\n",
    "        print(\"First 10 feature values:\", feat[:10])\n",
    "    else:\n",
    "        print(f\"Feature file missing for {img_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc06364",
   "metadata": {},
   "source": [
    "Cell 6 — Prepare captions: add <start> and <end>, build tokenizer\n",
    "What this does: for each caption we add <start> and <end>, then fit a Keras tokenizer (word → integer). We save tokenizer to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45b9c65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\yazda\\AppData\\Local\\Temp\\ipykernel_40296\\255802669.py:18: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token='<unk>', filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions available: 591753\n",
      "Vocab size (used): 5000\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - prepare captions and tokenizer\n",
    "all_captions = []\n",
    "image_ids = []   # parallel list: each caption has the image id it belongs to\n",
    "\n",
    "for fname, caps in img2caps.items():\n",
    "    image_id = os.path.splitext(fname)[0]\n",
    "    feat_file = os.path.join(FEATURES_DIR, image_id + '.npy')\n",
    "    if not os.path.exists(feat_file):\n",
    "        continue\n",
    "    for c in caps:\n",
    "        text = '<start> ' + c + ' <end>'\n",
    "        all_captions.append(text)\n",
    "        image_ids.append(image_id)\n",
    "\n",
    "print(\"Total captions available:\", len(all_captions))\n",
    "\n",
    "# tokenizer: convert words to integers\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token='<unk>', filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "with open(TOKENIZER_PATH, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "vocab_size = min(MAX_VOCAB, len(tokenizer.word_index) + 1)\n",
    "print(\"Vocab size (used):\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed3322",
   "metadata": {},
   "source": [
    "Cell 7 — Convert captions to sequences and make training lists\n",
    "What this does: converts caption strings to integer sequences and pairs them with their image feature ids. This prepares the data we will feed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2799f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 94629 Validation images: 23658\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - convert captions to sequences and prepare training examples\n",
    "sequences = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "# Truncate or pad later; but first map image -> list of sequences\n",
    "from collections import defaultdict\n",
    "imgid2seqs = defaultdict(list)\n",
    "for img_id, seq in zip(image_ids, sequences):\n",
    "    if len(seq) > MAX_LENGTH:\n",
    "        seq = seq[:MAX_LENGTH]  # simple truncation for very long captions\n",
    "    imgid2seqs[img_id].append(seq)\n",
    "\n",
    "# train/val split by image ids\n",
    "img_ids = list(imgid2seqs.keys())\n",
    "random.shuffle(img_ids)\n",
    "split = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:split]\n",
    "val_ids = img_ids[split:]\n",
    "print(\"Train images:\", len(train_ids), \"Validation images:\", len(val_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd59db7",
   "metadata": {},
   "source": [
    "Cell 8 — Simple data generator (yields batches)\n",
    "\n",
    "What this does: creates a generator that yields batches of image feature vectors, input sequences (padded), and target sequences (padded). We use \"teacher forcing\" where the model sees the true preceding words during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cf8d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replacement Cell 8: CaptionSequence (use instead of data_generator) ---\n",
    "import math\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class CaptionSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence to yield batches of ([features, in_seqs], out_seqs)\n",
    "    Loads image features from .npy files on the fly to keep memory low.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_ids, batch_size=BATCH_SIZE, shuffle=True):\n",
    "        self.image_ids = list(image_ids)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        # build list of samples: (img_id, in_padded, out_padded)\n",
    "        self.samples = []\n",
    "        for img_id in self.image_ids:\n",
    "            feat_path = os.path.join(FEATURES_DIR, img_id + '.npy')\n",
    "            if not os.path.exists(feat_path):\n",
    "                continue\n",
    "            for seq in imgid2seqs[img_id]:\n",
    "                in_seq = seq[:-1]\n",
    "                out_seq = seq[1:]\n",
    "                in_padded = pad_sequences([in_seq], maxlen=MAX_LENGTH, padding='post')[0]\n",
    "                out_padded = pad_sequences([out_seq], maxlen=MAX_LENGTH, padding='post')[0]\n",
    "                self.samples.append((img_id, in_padded, out_padded))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.samples) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # build batch\n",
    "        batch_samples = self.samples[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        X_feats = []\n",
    "        X_seq = []\n",
    "        Y_seq = []\n",
    "        for (img_id, in_padded, out_padded) in batch_samples:\n",
    "            feat = np.load(os.path.join(FEATURES_DIR, img_id + '.npy'))\n",
    "            X_feats.append(feat)\n",
    "            X_seq.append(in_padded)\n",
    "            Y_seq.append(out_padded)\n",
    "        # convert to numpy arrays\n",
    "        X_feats = np.array(X_feats)           # shape (B, feature_dim)\n",
    "        X_seq = np.array(X_seq)               # shape (B, MAX_LENGTH)\n",
    "        Y_seq = np.array(Y_seq)               # shape (B, MAX_LENGTH)\n",
    "        return [X_feats, X_seq], Y_seq\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f322a14",
   "metadata": {},
   "source": [
    "Cell 9 — Build a simple model (encoder = feature vector; decoder = embedding + LSTM)\n",
    "\n",
    "What this does: defines the Keras model. We take image features, pass them through a dense layer, concatenate them to each timestep embedding, then LSTM predicts a distribution over words for every timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d20bac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SimpleImageCaptionModel_v2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SimpleImageCaptionModel_v2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_features      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_seq           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feat_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ image_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ input_seq[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feat_tiled          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ feat_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_feat_token   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ feat_tiled[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │ concat_feat_toke… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_dense        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285,000</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_features      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_seq           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feat_dense (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ image_features[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,280,000\u001b[0m │ input_seq[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ feat_tiled          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ feat_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_feat_token   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ feat_tiled[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m787,456\u001b[0m │ concat_feat_toke… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_dense        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m5000\u001b[0m)  │  \u001b[38;5;34m1,285,000\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,877,000</span> (14.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,877,000\u001b[0m (14.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,877,000</span> (14.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,877,000\u001b[0m (14.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replacement Cell 9 - re-build model with mask_zero=False and ready for custom masked loss\n",
    "from tensorflow.keras.layers import RepeatVector, Concatenate, Dropout\n",
    "\n",
    "# get feature vector size from one saved .npy file\n",
    "sample_feat_file = next(iter(glob(os.path.join(FEATURES_DIR, '*.npy'))))\n",
    "feature_dim = np.load(sample_feat_file).shape[0]\n",
    "vocab_size_used = vocab_size  # computed earlier\n",
    "\n",
    "# Model inputs\n",
    "feat_input = Input(shape=(feature_dim,), name='image_features')   # one vector per image\n",
    "seq_input = Input(shape=(MAX_LENGTH,), name='input_seq')         # token sequence input\n",
    "\n",
    "# Map image feature to EMBED_DIM and repeat it MAX_LENGTH times using RepeatVector\n",
    "feat_dense = Dense(EMBED_DIM, activation='relu', name='feat_dense')(feat_input)  # (batch, EMBED_DIM)\n",
    "feat_tiled = RepeatVector(MAX_LENGTH, name='feat_tiled')(feat_dense)            # (batch, MAX_LENGTH, EMBED_DIM)\n",
    "\n",
    "# Embedding for tokens - IMPORTANT: mask_zero=False to avoid mask broadcasting issues\n",
    "emb = Embedding(vocab_size_used, EMBED_DIM, mask_zero=False, name='token_embedding')(seq_input)  # (batch,MAX,embed)\n",
    "\n",
    "# Concatenate feature (tiled) with token embedding at each timestep\n",
    "decoder_input = Concatenate(axis=-1, name='concat_feat_token')([emb, feat_tiled])  # (batch,MAX, 2*EMBED)\n",
    "\n",
    "# LSTM and output\n",
    "lstm = LSTM(UNITS, return_sequences=True, name='decoder_lstm')(decoder_input)\n",
    "drop = Dropout(0.5, name='dropout')(lstm)\n",
    "outputs = Dense(vocab_size_used, activation='softmax', name='output_dense')(drop)\n",
    "\n",
    "model = Model(inputs=[feat_input, seq_input], outputs=outputs, name='SimpleImageCaptionModel_v2')\n",
    "\n",
    "# Define masked loss (ignores padding token 0 in y_true)\n",
    "import tensorflow as tf\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "def masked_loss(y_true, y_pred):\n",
    "    # y_true: (batch, MAX_LENGTH), y_pred: (batch, MAX_LENGTH, vocab)\n",
    "    loss_ = loss_object(y_true, y_pred)                    # (batch, MAX_LENGTH)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)    # 1.0 for real tokens, 0.0 for padding\n",
    "    loss_ = loss_ * mask\n",
    "    # avoid dividing by zero\n",
    "    denom = tf.reduce_sum(mask)\n",
    "    return tf.reduce_sum(loss_) / (denom + 1e-7)\n",
    "\n",
    "model.compile(optimizer=Adam(LEARNING_RATE), loss=masked_loss)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c18c6",
   "metadata": {},
   "source": [
    "Cell 10 — Train the model (quick run)\n",
    "\n",
    "What this does: creates training and validation generators and runs .fit() for a few epochs. Keep EPOCHS small at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "167db240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_DIM detected: 2048\n",
      "Feature batch shape: (64, 2048)\n",
      "Input seq batch shape: (64, 30)\n",
      "Target batch shape: (64, 30)\n",
      "\n",
      "Found checkpoint: saved_models_tf\\epoch_5.keras (saved epoch 5)\n",
      "Loaded checkpoint with compile=True (optimizer state restored).\n",
      "Resuming training from the next epoch: epoch 6\n",
      "\n",
      "===== DATASET SUMMARY =====\n",
      "Images in training split:    94629\n",
      "Captions per image:          5\n",
      "Total caption samples:       473145\n",
      "Batch size:                  64\n",
      "→ Steps per epoch:           7393\n",
      "============================\n",
      "\n",
      "Epoch 6/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - loss: 2.8035INFO:tensorflow:Assets written to: saved_models_tf\\epoch_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_6'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_6\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_6.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4457s\u001b[0m 602ms/step - loss: 2.8035 - val_loss: 2.6241\n",
      "Epoch 7/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step - loss: 2.7531INFO:tensorflow:Assets written to: saved_models_tf\\epoch_7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_7'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_7\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_7.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3914s\u001b[0m 529ms/step - loss: 2.7531 - val_loss: 2.5875\n",
      "Epoch 8/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 2.7112INFO:tensorflow:Assets written to: saved_models_tf\\epoch_8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_8'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_8\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_8.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4386s\u001b[0m 593ms/step - loss: 2.7112 - val_loss: 2.5598\n",
      "Epoch 9/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - loss: 2.6770INFO:tensorflow:Assets written to: saved_models_tf\\epoch_9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_9'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_9\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_9.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4047s\u001b[0m 547ms/step - loss: 2.6770 - val_loss: 2.5369\n",
      "Epoch 10/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - loss: 2.6489INFO:tensorflow:Assets written to: saved_models_tf\\epoch_10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_10\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_10'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_10\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_10.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3444s\u001b[0m 466ms/step - loss: 2.6489 - val_loss: 2.5199\n",
      "Epoch 11/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376ms/step - loss: 2.6236INFO:tensorflow:Assets written to: saved_models_tf\\epoch_11\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_11\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_11'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_11\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_11.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3390s\u001b[0m 459ms/step - loss: 2.6236 - val_loss: 2.5019\n",
      "Epoch 12/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - loss: 2.6019INFO:tensorflow:Assets written to: saved_models_tf\\epoch_12\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_12\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_12'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_12\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_12.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3372s\u001b[0m 456ms/step - loss: 2.6019 - val_loss: 2.4874\n",
      "Epoch 13/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - loss: 2.5819INFO:tensorflow:Assets written to: saved_models_tf\\epoch_13\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_13\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_13'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_13\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_13.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4351s\u001b[0m 589ms/step - loss: 2.5819 - val_loss: 2.4744\n",
      "Epoch 14/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - loss: 2.5642INFO:tensorflow:Assets written to: saved_models_tf\\epoch_14\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_14\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_14'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_14\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_14.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3765s\u001b[0m 509ms/step - loss: 2.5642 - val_loss: 2.4660\n",
      "Epoch 15/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - loss: 2.5490INFO:tensorflow:Assets written to: saved_models_tf\\epoch_15\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_15\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_15'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_15\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_15.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3575s\u001b[0m 484ms/step - loss: 2.5490 - val_loss: 2.4561\n",
      "Epoch 16/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - loss: 2.5333INFO:tensorflow:Assets written to: saved_models_tf\\epoch_16\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_16\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_16'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_16\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_16.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3396s\u001b[0m 459ms/step - loss: 2.5333 - val_loss: 2.4461\n",
      "Epoch 17/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - loss: 2.5202INFO:tensorflow:Assets written to: saved_models_tf\\epoch_17\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_17\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_17'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_17\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_17.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3409s\u001b[0m 461ms/step - loss: 2.5202 - val_loss: 2.4395\n",
      "Epoch 18/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - loss: 2.5079INFO:tensorflow:Assets written to: saved_models_tf\\epoch_18\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_18\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_18'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_18\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_18.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3251s\u001b[0m 440ms/step - loss: 2.5079 - val_loss: 2.4341\n",
      "Epoch 19/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - loss: 2.4964INFO:tensorflow:Assets written to: saved_models_tf\\epoch_19\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_19\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_19'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_19\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_19.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3339s\u001b[0m 452ms/step - loss: 2.4964 - val_loss: 2.4267\n",
      "Epoch 20/20\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 2.4862INFO:tensorflow:Assets written to: saved_models_tf\\epoch_20\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models_tf\\epoch_20\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_models_tf\\epoch_20'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='image_features'), TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_seq')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30, 5000), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2298205755280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205755856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298240760528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205754128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2298205756240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Exported TF artifact: saved_models_tf\\epoch_20\n",
      "Saved .keras checkpoint: saved_models_tf\\epoch_20.keras\n",
      "\u001b[1m7393/7393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5907s\u001b[0m 799ms/step - loss: 2.4862 - val_loss: 2.4224\n",
      "Training finished and model saved to: ./caption_model.keras\n"
     ]
    }
   ],
   "source": [
    "# === CELL 10 (FINAL — with robust validation repeat) ===\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------------------\n",
    "# Detect feature dim\n",
    "# -------------------------\n",
    "sample_feat_path_iter = iter(glob.glob(os.path.join(FEATURES_DIR, '*.npy')))\n",
    "sample_feat = np.load(next(sample_feat_path_iter))\n",
    "FEATURE_DIM = sample_feat.shape[0]\n",
    "print(\"FEATURE_DIM detected:\", FEATURE_DIM)\n",
    "\n",
    "# -------------------------\n",
    "# Data generator\n",
    "# -------------------------\n",
    "def gen_for_ids(image_id_list):\n",
    "    for img_id in image_id_list:\n",
    "        feat_path = os.path.join(FEATURES_DIR, img_id + '.npy')\n",
    "        if not os.path.exists(feat_path):\n",
    "            continue\n",
    "        feat = np.load(feat_path).astype('float32')\n",
    "        for seq in imgid2seqs[img_id]:\n",
    "            in_seq = seq[:-1]\n",
    "            out_seq = seq[1:]\n",
    "            in_padded = pad_sequences([in_seq], maxlen=MAX_LENGTH, padding='post')[0].astype('int32')\n",
    "            out_padded = pad_sequences([out_seq], maxlen=MAX_LENGTH, padding='post')[0].astype('int32')\n",
    "            yield feat, in_padded, out_padded\n",
    "\n",
    "# TensorSpecs\n",
    "feature_spec = tf.TensorSpec(shape=(FEATURE_DIM,), dtype=tf.float32)\n",
    "inseq_spec = tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
    "outseq_spec = tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
    "output_signature = (feature_spec, inseq_spec, outseq_spec)\n",
    "\n",
    "# Build initial datasets (will be rebuilt before fit)\n",
    "train_ds = tf.data.Dataset.from_generator(lambda: gen_for_ids(train_ids), output_signature=output_signature)\n",
    "val_ds   = tf.data.Dataset.from_generator(lambda: gen_for_ids(val_ids),   output_signature=output_signature)\n",
    "\n",
    "def map_fn(feat, inseq, outseq):\n",
    "    return ((feat, inseq), outseq)\n",
    "\n",
    "train_ds = train_ds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Sanity check\n",
    "for (x_batch, y_batch) in train_ds.take(1):\n",
    "    print(\"Feature batch shape:\", x_batch[0].shape)\n",
    "    print(\"Input seq batch shape:\", x_batch[1].shape)\n",
    "    print(\"Target batch shape:\", y_batch.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Checkpoint discovery\n",
    "# -------------------------\n",
    "CKPT_DIR = 'saved_models_tf'\n",
    "latest_epoch = 0\n",
    "latest_path = None\n",
    "\n",
    "if os.path.exists(CKPT_DIR):\n",
    "    for name in os.listdir(CKPT_DIR):\n",
    "        m = re.match(r'epoch_(\\d+)(?:\\.keras)?$', name)\n",
    "        if m:\n",
    "            epoch_num = int(m.group(1))\n",
    "            candidate = os.path.join(CKPT_DIR, name)\n",
    "            # prefer single-file .keras if present\n",
    "            if name.endswith('.keras'):\n",
    "                if epoch_num >= latest_epoch:\n",
    "                    latest_epoch = epoch_num\n",
    "                    latest_path = candidate\n",
    "            else:\n",
    "                keras_sibling = candidate + '.keras'\n",
    "                if os.path.exists(keras_sibling) and epoch_num >= latest_epoch:\n",
    "                    latest_epoch = epoch_num\n",
    "                    latest_path = keras_sibling\n",
    "\n",
    "# -------------------------\n",
    "# Load checkpoint + RERUN option\n",
    "# -------------------------\n",
    "initial_epoch = 0\n",
    "\n",
    "if latest_path is not None and not START_FRESH:\n",
    "    print(f\"\\nFound checkpoint: {latest_path} (saved epoch {latest_epoch})\")\n",
    "    # Try to load with compile=True to restore optimizer & training config\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(latest_path, compile=True, custom_objects={\"masked_loss\": masked_loss})\n",
    "        print(\"Loaded checkpoint with compile=True (optimizer state restored).\")\n",
    "    except Exception as e:\n",
    "        print(\"Load with compile=True failed:\", repr(e))\n",
    "        print(\"Falling back to load_model(..., compile=False) and manual compile.\")\n",
    "        model = tf.keras.models.load_model(latest_path, compile=False)\n",
    "        # safe manual compile fallback\n",
    "        try:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "            loss_fn = masked_loss\n",
    "        except Exception:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "            print(\"masked_loss not found — using SparseCategoricalCrossentropy fallback for manual compile.\")\n",
    "        model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n",
    "        print(\"Loaded without optimizer state; model compiled manually.\")\n",
    "\n",
    "    # RERUN option: set to True to re-run the saved epoch, False to resume next epoch\n",
    "    RERUN_LAST = False   # <-- set False for normal resume (start from next epoch)\n",
    "    if RERUN_LAST:\n",
    "        initial_epoch = max(0, latest_epoch - 1)\n",
    "    else:\n",
    "        initial_epoch = latest_epoch\n",
    "\n",
    "    # Friendly message\n",
    "    if RERUN_LAST:\n",
    "        print(f\"Re-running the most recently completed epoch: epoch {initial_epoch + 1}\")\n",
    "    else:\n",
    "        print(f\"Resuming training from the next epoch: epoch {initial_epoch + 1}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo checkpoint found or START_FRESH=True. Starting from scratch.\")\n",
    "    initial_epoch = 0\n",
    "\n",
    "# -------------------------\n",
    "# Training / steps per epoch (dynamic)\n",
    "# -------------------------\n",
    "try:\n",
    "    num_samples = len(train_ids) * 5   # 5 captions per image\n",
    "    num_images = len(train_ids)\n",
    "except Exception:\n",
    "    num_images = 5000\n",
    "    num_samples = num_images * 5\n",
    "\n",
    "STEPS_PER_EPOCH = math.ceil(num_samples / BATCH_SIZE)\n",
    "\n",
    "print(\"\\n===== DATASET SUMMARY =====\")\n",
    "print(f\"Images in training split:    {num_images}\")\n",
    "print(f\"Captions per image:          5\")\n",
    "print(f\"Total caption samples:       {num_samples}\")\n",
    "print(f\"Batch size:                  {BATCH_SIZE}\")\n",
    "print(f\"→ Steps per epoch:           {STEPS_PER_EPOCH}\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "# If initial_epoch >= EPOCHS we skip training (already finished)\n",
    "if initial_epoch >= EPOCHS:\n",
    "    print(f\"Checkpoint shows training already complete up to epoch {initial_epoch}. No further training.\")\n",
    "    try:\n",
    "        model.save(MODEL_PATH)\n",
    "        print(\"Final model saved to:\", MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"Final save failed:\", e)\n",
    "else:\n",
    "    # ---------- Rebuild train_ds and val_ds right before training (prevents earlier iterations from exhausting them) ----------\n",
    "    train_ds = tf.data.Dataset.from_generator(lambda: gen_for_ids(train_ids), output_signature=output_signature)\n",
    "    train_ds = train_ds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_generator(lambda: gen_for_ids(val_ids), output_signature=output_signature)\n",
    "    val_ds = val_ds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # ---------- Optional quick debug: count batches available (without repeat) ----------\n",
    "    # debug_count = 0\n",
    "    # for _ in train_ds.take(10000):\n",
    "    #     debug_count += 1\n",
    "    # print(f\"DEBUG: batches available (without repeat): {debug_count}  — expected: {STEPS_PER_EPOCH}\")\n",
    "\n",
    "    # if debug_count < STEPS_PER_EPOCH:\n",
    "    #     print(\"⚠ Note: dataset produces fewer batches than expected. .repeat() will fill remaining steps_per_epoch.\")\n",
    "\n",
    "    # ---------- Fit using repeat() so epochs always have STEPS_PER_EPOCH batches and val stable ----------\n",
    "    VAL_SAMPLES = len(val_ids) * 5\n",
    "    VALIDATION_STEPS = math.ceil(VAL_SAMPLES / BATCH_SIZE)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds.repeat(),            # ensures the dataset restarts if it ends early\n",
    "        validation_data=val_ds.repeat(),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        epochs=EPOCHS,\n",
    "        initial_epoch=initial_epoch,\n",
    "        callbacks=[save_callback],\n",
    "        steps_per_epoch=STEPS_PER_EPOCH\n",
    "    )\n",
    "\n",
    "    # ---------- Final save ----------\n",
    "    try:\n",
    "        model.save(MODEL_PATH)\n",
    "        print(\"Training finished and model saved to:\", MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"Final save failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634aaf2",
   "metadata": {},
   "source": [
    "Cell 11 — Helper: convert sequence of token ids back to text\n",
    "\n",
    "What this does: loads tokenizer and defines a utility to convert numeric sequences back to readable words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11c76b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 - load tokenizer and helper function\n",
    "with open(TOKENIZER_PATH, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "def seq_to_text(seq):\n",
    "    words = []\n",
    "    for idx in seq:\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        word = index_word.get(idx, '')\n",
    "        if word == '<end>':\n",
    "            break\n",
    "        words.append(word)\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5788d",
   "metadata": {},
   "source": [
    "Cell 12 — Inference: greedy caption generation for one image\n",
    "\n",
    "What this does: given an image file id (e.g., '000000012345'), this function generates a caption word-by-word greedily (chooses most probable next word). Example usage included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c708b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 000000061675\n",
      "Generated caption: a herd of sheep grazing on a lush green field\n",
      "Reference captions: ['some white and black sheep are on some grass hills and trees', 'some sheep standing together while surrounded by some tall grass', 'a herd of sheep standing on a lush green hillside.']\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 - fixed greedy inference (uses full history)\n",
    "def generate_caption(image_id, max_len=MAX_LENGTH):\n",
    "    # 1) load feature\n",
    "    feat_path = os.path.join(FEATURES_DIR, image_id + '.npy')\n",
    "    if not os.path.exists(feat_path):\n",
    "        raise FileNotFoundError(f\"Feature file not found for: {feat_path}\")\n",
    "    feat = np.load(feat_path).astype('float32').reshape(1, -1)\n",
    "\n",
    "    # 2) setup tokens\n",
    "    start_id = tokenizer.word_index.get('<start>')\n",
    "    end_id   = tokenizer.word_index.get('<end>')\n",
    "    in_tokens = [start_id]   # sequence we feed (grows every step)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # pad current sequence\n",
    "        in_seq = pad_sequences([in_tokens], maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "        # 3) predict distribution over vocab for all timesteps\n",
    "        preds = model.predict([feat, in_seq], verbose=0)  # (1, MAX_LENGTH, vocab)\n",
    "\n",
    "        # we care about the last real timestep = len(in_tokens) - 1\n",
    "        step = len(in_tokens) - 1\n",
    "        token_probs = preds[0, step]           # shape (vocab_size,)\n",
    "        next_id = int(np.argmax(token_probs))  # greedy\n",
    "\n",
    "        # 4) stopping conditions\n",
    "        if next_id == 0:              # pad\n",
    "            break\n",
    "        if end_id is not None and next_id == end_id:\n",
    "            break\n",
    "\n",
    "        # 5) append token\n",
    "        in_tokens.append(next_id)\n",
    "\n",
    "    # 6) convert ids → words, skipping <start>/<end>/0\n",
    "    words = []\n",
    "    for idx in in_tokens[1:]:  # skip <start>\n",
    "        if idx == 0:\n",
    "            break\n",
    "        if end_id is not None and idx == end_id:\n",
    "            break\n",
    "        w = index_word.get(idx, '')\n",
    "        if w in ('<start>', '<end>', ''):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Quick test on a random validation image\n",
    "if len(val_ids) > 0:\n",
    "    test_img = random.choice(val_ids)\n",
    "    print(\"Image ID:\", test_img)\n",
    "    print(\"Generated caption:\", generate_caption(test_img))\n",
    "    print(\"Reference captions:\", img2caps.get(test_img + '.jpg', [])[:3])\n",
    "else:\n",
    "    print(\"No validation images available for test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7970605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded punkt_tab and punkt. Now re-run your BLEU evaluation cell.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\yazda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yazda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')   # downloads the punkt_tab data\n",
    "nltk.download('punkt')       # safe to run again (already up-to-date)\n",
    "print(\"Downloaded punkt_tab and punkt. Now re-run your BLEU evaluation cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41c58b",
   "metadata": {},
   "source": [
    "Cell 13 — Quick BLEU evaluation (small sample)\n",
    "\n",
    "What this does: computes BLEU-1 and BLEU-4 on a small sample of validation images to get a rough numeric measure of quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3af524e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.6424, BLEU-4: 0.2137\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 - quick BLEU check on small sample\n",
    "def evaluate_bleu(n=50):\n",
    "    sample = random.sample(val_ids, min(n, len(val_ids)))\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    for img_id in sample:\n",
    "        references = [nltk.word_tokenize(c) for c in img2caps.get(img_id + '.jpg', [])]\n",
    "        pred = generate_caption(img_id)\n",
    "        hyp = nltk.word_tokenize(pred)\n",
    "        refs.append(references)\n",
    "        hyps.append(hyp)\n",
    "    bleu1 = corpus_bleu(refs, hyps, weights=(1,0,0,0))\n",
    "    bleu4 = corpus_bleu(refs, hyps, weights=(0.25,0.25,0.25,0.25))\n",
    "    print(f\"BLEU-1: {bleu1:.4f}, BLEU-4: {bleu4:.4f}\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_bleu(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0c4f1",
   "metadata": {},
   "source": [
    "Cell 14 — Save tokenizer and final notes\n",
    "\n",
    "What this does: ensures tokenizer is saved for later use and gives tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feb7552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to ./tokenizer.pickle\n",
      "Done. Tips:\n",
      "- Increase EPOCHS and MAX_VOCAB for better results.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14 - save tokenizer\n",
    "with open(TOKENIZER_PATH, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer saved to\", TOKENIZER_PATH)\n",
    "print(\"Done. Tips:\\n- Increase EPOCHS and MAX_VOCAB for better results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
